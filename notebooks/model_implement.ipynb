{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bb9ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "\n",
    "import random, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision import models\n",
    "from torchvision.transforms import InterpolationMode as IM\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(r\"C:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\coralscapesdata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9db7e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# converts any input image into an RGB PIL image\n",
    "def to_pil(img):\n",
    "    # if its already PIL, keep it\n",
    "    if isinstance(img, Image.Image): \n",
    "        return img\n",
    "    # otherwise, make it an array\n",
    "    arr = np.asarray(img)\n",
    "    # if its a 2d gray scale image without color, make it grayscale PIL\n",
    "    if arr.ndim == 2: \n",
    "        return Image.fromarray(arr.astype(np.uint8))\n",
    "    # convert to PIL data type\n",
    "    if arr.dtype != np.uint8:\n",
    "        arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "    # convert values in array to valid rgb range and the convert to PIL-compatible type\n",
    "    # If image array is not in the 8 bit range, clip all values to the valid 8-bit range for PIL handling\n",
    "    if arr.ndim == 3 and arr.shape[2] == 4:  # RGBA -> RGB\n",
    "        arr = arr[:, :, :3]\n",
    "    # if images are RGBA, with an alpha transparency channel, drop that and make it sole RGB\n",
    "    return Image.fromarray(arr)\n",
    "    # turn array into PIL object\n",
    "\n",
    "# convert segmentation masks into grayscale PIL images \n",
    "def mask_to_pil(mask): \n",
    "    if isinstance(mask, Image.Image):\n",
    "        return mask.convert(\"L\")\n",
    "    # if mask is already PIL, make sure it's in grayscale mode \n",
    "    arr = np.asarray(mask)\n",
    "    if arr.dtype != np.uint8:\n",
    "        arr = arr.astype(np.uint8)\n",
    "    # array should be numpy and 8-bit        \n",
    "    return Image.fromarray(arr, mode=\"L\") # return PIL grayscale image\n",
    "\n",
    "# function to retrieve and confirm the number of seg. classes in dataset \n",
    "# sample limit: only scans first N integers for time saving\n",
    "# hf train split selects the example: ex. dataset['train']\n",
    "def get_num_classes(hf_train_split, sample_limit=None):\n",
    "    \"\"\"Scan masks to find max label and infer num_classes.\"\"\"\n",
    "    max_label = 0 # placeholder to keep track of max class \n",
    "    # How many samples to go over, all if sample limit is none\n",
    "    N = len(hf_train_split) if sample_limit is None else min(sample_limit, len(hf_train_split))\n",
    "    for i in range(N): \n",
    "        m = hf_train_split[i][\"label\"]\n",
    "        m = np.asarray(m)\n",
    "        # max label gets the largest pixel segmentation value found for the label it is cycling through\n",
    "        max_label = max(max_label, int(m.max()))\n",
    "    return max_label + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "441b6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEGFORMER GOALS:\n",
    "# 1. Every image and mask are the same size\n",
    "# 2. Training data gets random augmentations (to improve generalization)\n",
    "# 3. Validation/testing data is preprocessed deterministically (no randomness)\n",
    "# 4. Both image and mask transformations stay perfectly aligned pixel-for-pixel\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as TF, InterpolationMode as IM\n",
    "\n",
    "class SegTransform:\n",
    "    def __init__(self, size=512, crop_size=512, is_train=True):\n",
    "        self.size = size\n",
    "        self.crop_size = crop_size\n",
    "        self.is_train = is_train\n",
    "        self.color_jitter = T.ColorJitter(\n",
    "            brightness=0.2,  # 20% brighter/darker\n",
    "            contrast=0.2,    # 20% contrast variation\n",
    "            saturation=0.2,  # 20% saturation shift\n",
    "            hue=0.05         # small hue jitter\n",
    "        )\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        img = to_pil(img).convert(\"RGB\")\n",
    "        mask = mask_to_pil(mask)\n",
    "\n",
    "        # Resize\n",
    "        img  = TF.resize(img,  self.size, interpolation=IM.BILINEAR)\n",
    "        mask = TF.resize(mask, self.size, interpolation=IM.NEAREST)\n",
    "\n",
    "        if self.is_train and self.crop_size is not None:\n",
    "            # >>> this is the correct way <<<\n",
    "            i, j, h, w = T.RandomCrop.get_params(img, output_size=(self.crop_size, self.crop_size))\n",
    "            img  = TF.crop(img,  i, j, h, w)\n",
    "            mask = TF.crop(mask, i, j, h, w)\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                img  = TF.hflip(img)\n",
    "                mask = TF.hflip(mask)\n",
    "            \n",
    "            img = self.color_jitter(img)\n",
    "\n",
    "\n",
    "        else:\n",
    "            if self.crop_size is not None and self.crop_size < self.size:\n",
    "                img  = TF.center_crop(img,  self.crop_size)\n",
    "                mask = TF.center_crop(mask, self.crop_size)\n",
    "\n",
    "        img = TF.to_tensor(img)\n",
    "        img = TF.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        mask = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        return img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f6642e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFDatasetWrapper(Dataset):\n",
    "    def __init__(self, hf_split, transform: SegTransform):\n",
    "        self.ds = hf_split\n",
    "        self.t = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        img, mask = row[\"image\"], row[\"label\"]\n",
    "        img, mask = self.t(img, mask)\n",
    "        return img, mask\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, masks = zip(*batch)\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    masks = torch.stack(masks, dim=0)\n",
    "    return imgs, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d0b612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_metrics(logits, targets, num_classes):\n",
    "    \"\"\"\n",
    "    Compute pixel accuracy and mIoU for a batch.\n",
    "    logits: (B, C, H, W) raw\n",
    "    targets: (B, H, W) long\n",
    "    \"\"\"\n",
    "    preds = logits.argmax(1)  # (B,H,W)\n",
    "    valid = (targets >= 0)  # assume all valid; adjust if you use ignore_index\n",
    "    correct = (preds[valid] == targets[valid]).sum().item()\n",
    "    total = valid.sum().item()\n",
    "    pix_acc = correct / max(1, total)\n",
    "\n",
    "    # IoU per class\n",
    "    ious = []\n",
    "    for c in range(num_classes):\n",
    "        pred_c = (preds == c)\n",
    "        targ_c = (targets == c)\n",
    "        inter = (pred_c & targ_c).sum().item()\n",
    "        union = (pred_c | targ_c).sum().item()\n",
    "        if union == 0:\n",
    "            continue\n",
    "        ious.append(inter / union)\n",
    "    miou = float(np.mean(ious)) if ious else 0.0\n",
    "    return pix_acc, miou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2c3cd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Inferred num_classes: 40\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Infer class count (full scan, set sample_limit to speed up if huge)\n",
    "num_classes = get_num_classes(dataset[\"train\"], sample_limit=None)\n",
    "print(\"Inferred num_classes:\", num_classes)\n",
    "\n",
    "# Transforms (tweak sizes to fit GPU)\n",
    "TRAIN_SIZE = 320  #544\n",
    "CROP = 320   #512f\n",
    "train_tf = SegTransform(size=TRAIN_SIZE, crop_size=CROP, is_train=True)\n",
    "val_tf   = SegTransform(size=TRAIN_SIZE, crop_size=CROP, is_train=False)\n",
    "test_tf  = SegTransform(size=TRAIN_SIZE, crop_size=CROP, is_train=False)\n",
    "\n",
    "train_set = HFDatasetWrapper(dataset[\"train\"], train_tf)\n",
    "val_set   = HFDatasetWrapper(dataset[\"validation\"], val_tf)\n",
    "test_set  = HFDatasetWrapper(dataset[\"test\"], test_tf)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=3, shuffle=True, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_set,   batch_size=4, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_set,  batch_size=4, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "955ad509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose backbone\n",
    "model = models.segmentation.deeplabv3_resnet50(num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "# Optimizer & scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=3e-4, steps_per_epoch=len(train_loader), epochs= 30\n",
    ")\n",
    "\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b59660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss, total_pix, total_iou, total_batches = 0.0, 0.0, 0.0, 0\n",
    "    for imgs, masks in loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            out = model(imgs)[\"out\"]\n",
    "            loss = criterion(out, masks)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "        pix_acc, miou = segmentation_metrics(out.detach().cpu(), masks.cpu(), num_classes)\n",
    "        total_loss += loss.item()\n",
    "        total_pix += pix_acc\n",
    "        total_iou += miou\n",
    "        total_batches += 1\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / max(1, total_batches),\n",
    "        \"pix_acc\": total_pix / max(1, total_batches),\n",
    "        \"miou\": total_iou / max(1, total_batches),    \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fef954c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as TF, InterpolationMode as IM\n",
    "\n",
    "class SegTransform:\n",
    "    def __init__(self, size=512, crop_size=512, is_train=True):\n",
    "        self.size = size\n",
    "        self.crop_size = crop_size\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        img  = to_pil(img).convert(\"RGB\")\n",
    "        mask = mask_to_pil(mask)\n",
    "\n",
    "        img  = TF.resize(img,  self.size, interpolation=IM.BILINEAR)\n",
    "        mask = TF.resize(mask, self.size, interpolation=IM.NEAREST)\n",
    "\n",
    "        if self.is_train and self.crop_size is not None:\n",
    "            i, j, h, w = T.RandomCrop.get_params(img, (self.crop_size, self.crop_size))\n",
    "            img  = TF.crop(img,  i, j, h, w)\n",
    "            mask = TF.crop(mask, i, j, h, w)\n",
    "            if random.random() < 0.5:\n",
    "                img  = TF.hflip(img)\n",
    "                mask = TF.hflip(mask)\n",
    "        elif self.crop_size and self.crop_size < self.size:\n",
    "            img  = TF.center_crop(img,  self.crop_size)\n",
    "            mask = TF.center_crop(mask, self.crop_size)\n",
    "\n",
    "        img  = TF.to_tensor(img)\n",
    "        img  = TF.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        mask = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        return img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2a463ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnav_vckkum5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/30] train: loss=2.8194 acc=0.346 mIoU=0.063 | val: loss=2.1522 acc=0.485 mIoU=0.096 | 5079.9s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.096)\n",
      "[02/30] train: loss=2.0596 acc=0.458 mIoU=0.130 | val: loss=1.9461 acc=0.462 mIoU=0.117 | 5116.5s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.117)\n",
      "[03/30] train: loss=1.7111 acc=0.502 mIoU=0.156 | val: loss=1.5180 acc=0.551 mIoU=0.143 | 5129.4s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.143)\n",
      "[04/30] train: loss=1.5937 acc=0.516 mIoU=0.167 | val: loss=1.7648 acc=0.477 mIoU=0.128 | 5166.0s\n",
      "[05/30] train: loss=1.5167 acc=0.529 mIoU=0.177 | val: loss=1.5519 acc=0.539 mIoU=0.144 | 5161.2s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.144)\n",
      "[06/30] train: loss=1.5163 acc=0.528 mIoU=0.178 | val: loss=1.7679 acc=0.477 mIoU=0.133 | 5186.4s\n",
      "[07/30] train: loss=1.4594 acc=0.541 mIoU=0.187 | val: loss=1.5664 acc=0.518 mIoU=0.146 | 5169.6s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.146)\n",
      "[08/30] train: loss=1.4604 acc=0.539 mIoU=0.186 | val: loss=1.6410 acc=0.500 mIoU=0.142 | 5167.9s\n",
      "[09/30] train: loss=1.4022 acc=0.554 mIoU=0.193 | val: loss=1.6736 acc=0.503 mIoU=0.146 | 5179.1s\n",
      "[10/30] train: loss=1.3949 acc=0.557 mIoU=0.195 | val: loss=1.7179 acc=0.487 mIoU=0.143 | 5166.4s\n",
      "[11/30] train: loss=1.3083 acc=0.576 mIoU=0.208 | val: loss=1.4094 acc=0.568 mIoU=0.160 | 5180.2s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.160)\n",
      "[12/30] train: loss=1.2965 acc=0.582 mIoU=0.208 | val: loss=2.0524 acc=0.480 mIoU=0.120 | 5214.3s\n",
      "[13/30] train: loss=1.2748 acc=0.587 mIoU=0.217 | val: loss=1.4325 acc=0.559 mIoU=0.168 | 5148.8s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.168)\n",
      "[14/30] train: loss=1.2516 acc=0.592 mIoU=0.217 | val: loss=1.3285 acc=0.574 mIoU=0.176 | 5142.6s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.176)\n",
      "[15/30] train: loss=1.1826 acc=0.610 mIoU=0.231 | val: loss=1.3813 acc=0.577 mIoU=0.165 | 5136.0s\n",
      "[16/30] train: loss=1.1317 acc=0.625 mIoU=0.243 | val: loss=1.4346 acc=0.561 mIoU=0.177 | 5136.0s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.177)\n",
      "[17/30] train: loss=1.1116 acc=0.631 mIoU=0.248 | val: loss=1.3187 acc=0.576 mIoU=0.180 | 5056.1s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.180)\n",
      "[18/30] train: loss=1.0755 acc=0.639 mIoU=0.256 | val: loss=1.3894 acc=0.556 mIoU=0.175 | 5048.3s\n",
      "[19/30] train: loss=1.0145 acc=0.658 mIoU=0.269 | val: loss=1.3309 acc=0.585 mIoU=0.194 | 5051.4s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.194)\n",
      "[20/30] train: loss=0.9991 acc=0.663 mIoU=0.275 | val: loss=1.3328 acc=0.578 mIoU=0.182 | 5048.5s\n",
      "[21/30] train: loss=0.9677 acc=0.672 mIoU=0.283 | val: loss=1.2601 acc=0.595 mIoU=0.201 | 5012.6s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.201)\n",
      "[22/30] train: loss=0.9259 acc=0.684 mIoU=0.294 | val: loss=1.2720 acc=0.591 mIoU=0.216 | 5044.1s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.216)\n",
      "[23/30] train: loss=0.8847 acc=0.696 mIoU=0.307 | val: loss=1.2530 acc=0.595 mIoU=0.207 | 5038.3s\n",
      "[24/30] train: loss=0.8459 acc=0.710 mIoU=0.320 | val: loss=1.1660 acc=0.625 mIoU=0.213 | 5065.1s\n",
      "[25/30] train: loss=0.8132 acc=0.719 mIoU=0.331 | val: loss=1.2128 acc=0.612 mIoU=0.215 | 5045.0s\n",
      "[26/30] train: loss=0.7991 acc=0.725 mIoU=0.335 | val: loss=1.2108 acc=0.609 mIoU=0.215 | 5068.6s\n",
      "[27/30] train: loss=0.7737 acc=0.733 mIoU=0.340 | val: loss=1.1592 acc=0.623 mIoU=0.217 | 5060.3s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.217)\n",
      "[28/30] train: loss=0.7649 acc=0.737 mIoU=0.347 | val: loss=1.1907 acc=0.616 mIoU=0.219 | 5104.9s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.219)\n",
      "[29/30] train: loss=0.7540 acc=0.740 mIoU=0.349 | val: loss=1.1718 acc=0.619 mIoU=0.220 | 5120.8s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.220)\n",
      "[30/30] train: loss=0.7548 acc=0.739 mIoU=0.347 | val: loss=1.1738 acc=0.620 mIoU=0.224 | 5068.6s\n",
      "  ↳ Saved checkpoint: c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth (best val mIoU 0.224)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30 # 25\n",
    "best_miou = -1\n",
    "save_path = r\"c:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\modelversions\\deeplabv3_resnet50_coral.pth\"\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    tr = run_epoch(model, train_loader, train=True)\n",
    "    va = run_epoch(model, val_loader, train=False)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[{epoch:02d}/{EPOCHS}] \"\n",
    "          f\"train: loss={tr['loss']:.4f} acc={tr['pix_acc']:.3f} mIoU={tr['miou']:.3f} | \"\n",
    "          f\"val: loss={va['loss']:.4f} acc={va['pix_acc']:.3f} mIoU={va['miou']:.3f} | \"\n",
    "          f\"{dt:.1f}s\")\n",
    "\n",
    "    # save best by val mIoU\n",
    "    if va[\"miou\"] > best_miou:\n",
    "        best_miou = va[\"miou\"]\n",
    "        torch.save({\"model\": model.state_dict(),\n",
    "                    \"num_classes\": num_classes}, save_path)\n",
    "        print(f\"  ↳ Saved checkpoint: {save_path} (best val mIoU {best_miou:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: loss=1.0980 acc=0.635 mIoU=0.230\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(save_path, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "test_metrics = run_epoch(model, test_loader, train=False)\n",
    "print(f\"TEST: loss={test_metrics['loss']:.4f} acc={test_metrics['pix_acc']:.3f} mIoU={test_metrics['miou']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b2b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
