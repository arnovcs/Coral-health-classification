{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb9ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "\n",
    "import random, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision import models\n",
    "from torchvision.transforms import InterpolationMode as IM\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(r\"C:\\Users\\arnav_vckkum5\\OneDrive\\coral-data\\coralscapesdata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db7e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# converts any input image into an RGB PIL image\n",
    "def to_pil(img):\n",
    "    # if its already PIL, keep it\n",
    "    if isinstance(img, Image.Image): \n",
    "        return img\n",
    "    # otherwise, make it an array\n",
    "    arr = np.asarray(img)\n",
    "    # if its a 2d gray scale image without color, make it grayscale PIL\n",
    "    if arr.ndim == 2: \n",
    "        return Image.fromarray(arr.astype(np.uint8))\n",
    "    # convert to PIL data type\n",
    "    if arr.dtype != np.uint8:\n",
    "        arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "    # convert values in array to valid rgb range and the convert to PIL-compatible type\n",
    "    # If image array is not in the 8 bit range, clip all values to the valid 8-bit range for PIL handling\n",
    "    if arr.ndim == 3 and arr.shape[2] == 4:  # RGBA -> RGB\n",
    "        arr = arr[:, :, :3]\n",
    "    # if images are RGBA, with an alpha transparency channel, drop that and make it sole RGB\n",
    "    return Image.fromarray(arr)\n",
    "    # turn array into PIL object\n",
    "\n",
    "# convert segmentation masks into grayscale PIL images \n",
    "def mask_to_pil(mask): \n",
    "    if isinstance(mask, Image.Image):\n",
    "        return mask.convert(\"L\")\n",
    "    # if mask is already PIL, make sure it's in grayscale mode \n",
    "    arr = np.asarray(mask)\n",
    "    if arr.dtype != np.uint8:\n",
    "        arr = arr.astype(np.uint8)\n",
    "    # array should be numpy and 8-bit        \n",
    "    return Image.fromarray(arr, mode=\"L\") # return PIL grayscale image\n",
    "\n",
    "# function to retrieve and confirm the number of seg. classes in dataset \n",
    "# sample limit: only scans first N integers for time saving\n",
    "# hf train split selects the example: ex. dataset['train']\n",
    "def get_num_classes(hf_train_split, sample_limit=None):\n",
    "    \"\"\"Scan masks to find max label and infer num_classes.\"\"\"\n",
    "    max_label = 0 # placeholder to keep track of max class \n",
    "    # How many samples to go over, all if sample limit is none\n",
    "    N = len(hf_train_split) if sample_limit is None else min(sample_limit, len(hf_train_split))\n",
    "    for i in range(N): \n",
    "        m = hf_train_split[i][\"label\"]\n",
    "        m = np.asarray(m)\n",
    "        # max label gets the largest pixel segmentation value found for the label it is cycling through\n",
    "        max_label = max(max_label, int(m.max()))\n",
    "    return max_label + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "441b6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEGFORMER GOALS:\n",
    "# 1. Every image and mask are the same size\n",
    "# 2. Training data gets random augmentations (to improve generalization)\n",
    "# 3. Validation/testing data is preprocessed deterministically (no randomness)\n",
    "# 4. Both image and mask transformations stay perfectly aligned pixel-for-pixel\n",
    "\n",
    "class SegTransform:\n",
    "    def __init__(self, size=512, crop_size=512, is_train=True):\n",
    "        self.size = size\n",
    "        self.crop_size = crop_size\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        img = to_pil(img).convert(\"RGB\")\n",
    "        mask = mask_to_pil(mask)\n",
    "\n",
    "        # Resize\n",
    "        img  = TF.resize(img,  self.size, interpolation=IM.BILINEAR)\n",
    "        mask = TF.resize(mask, self.size, interpolation=IM.NEAREST)\n",
    "\n",
    "        if self.is_train and self.crop_size is not None:\n",
    "            # >>> this is the correct way <<<\n",
    "            i, j, h, w = T.RandomCrop.get_params(img, output_size=(self.crop_size, self.crop_size))\n",
    "            img  = TF.crop(img,  i, j, h, w)\n",
    "            mask = TF.crop(mask, i, j, h, w)\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                img  = TF.hflip(img)\n",
    "                mask = TF.hflip(mask)\n",
    "        else:\n",
    "            if self.crop_size is not None and self.crop_size < self.size:\n",
    "                img  = TF.center_crop(img,  self.crop_size)\n",
    "                mask = TF.center_crop(mask, self.crop_size)\n",
    "\n",
    "        img = TF.to_tensor(img)\n",
    "        img = TF.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        mask = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        return img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6642e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFDatasetWrapper(Dataset):\n",
    "    def __init__(self, hf_split, transform: SegTransform):\n",
    "        self.ds = hf_split\n",
    "        self.t = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        img, mask = row[\"image\"], row[\"label\"]\n",
    "        img, mask = self.t(img, mask)\n",
    "        return img, mask\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, masks = zip(*batch)\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    masks = torch.stack(masks, dim=0)\n",
    "    return imgs, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d0b612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_metrics(logits, targets, num_classes):\n",
    "    \"\"\"\n",
    "    Compute pixel accuracy and mIoU for a batch.\n",
    "    logits: (B, C, H, W) raw\n",
    "    targets: (B, H, W) long\n",
    "    \"\"\"\n",
    "    preds = logits.argmax(1)  # (B,H,W)\n",
    "    valid = (targets >= 0)  # assume all valid; adjust if you use ignore_index\n",
    "    correct = (preds[valid] == targets[valid]).sum().item()\n",
    "    total = valid.sum().item()\n",
    "    pix_acc = correct / max(1, total)\n",
    "\n",
    "    # IoU per class\n",
    "    ious = []\n",
    "    for c in range(num_classes):\n",
    "        pred_c = (preds == c)\n",
    "        targ_c = (targets == c)\n",
    "        inter = (pred_c & targ_c).sum().item()\n",
    "        union = (pred_c | targ_c).sum().item()\n",
    "        if union == 0:\n",
    "            continue\n",
    "        ious.append(inter / union)\n",
    "    miou = float(np.mean(ious)) if ious else 0.0\n",
    "    return pix_acc, miou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2c3cd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Inferred num_classes: 40\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Infer class count (full scan, set sample_limit to speed up if huge)\n",
    "num_classes = get_num_classes(dataset[\"train\"], sample_limit=None)\n",
    "print(\"Inferred num_classes:\", num_classes)\n",
    "\n",
    "# Transforms (tweak sizes to fit GPU)\n",
    "TRAIN_SIZE = 320  #544\n",
    "CROP = 288   #512f\n",
    "train_tf = SegTransform(size=TRAIN_SIZE, crop_size=CROP, is_train=True)\n",
    "val_tf   = SegTransform(size=TRAIN_SIZE, crop_size=CROP, is_train=False)\n",
    "test_tf  = SegTransform(size=TRAIN_SIZE, crop_size=CROP, is_train=False)\n",
    "\n",
    "train_set = HFDatasetWrapper(dataset[\"train\"], train_tf)\n",
    "val_set   = HFDatasetWrapper(dataset[\"validation\"], val_tf)\n",
    "test_set  = HFDatasetWrapper(dataset[\"test\"], test_tf)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=3, shuffle=True, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_set,   batch_size=3, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_set,  batch_size=2, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "955ad509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose backbone\n",
    "model = models.segmentation.deeplabv3_resnet50(num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "# Optimizer & scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=3e-4, steps_per_epoch=len(train_loader), epochs=25\n",
    ")\n",
    "\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b59660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss, total_pix, total_iou, total_batches = 0.0, 0.0, 0.0, 0\n",
    "    for imgs, masks in loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            out = model(imgs)[\"out\"]\n",
    "            loss = criterion(out, masks)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "        pix_acc, miou = segmentation_metrics(out.detach().cpu(), masks.cpu(), num_classes)\n",
    "        total_loss += loss.item()\n",
    "        total_pix += pix_acc\n",
    "        total_iou += miou\n",
    "        total_batches += 1\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / max(1, total_batches),\n",
    "        \"pix_acc\": total_pix / max(1, total_batches),\n",
    "        \"miou\": total_iou / max(1, total_batches),    \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fef954c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as TF, InterpolationMode as IM\n",
    "\n",
    "class SegTransform:\n",
    "    def __init__(self, size=512, crop_size=512, is_train=True):\n",
    "        self.size = size\n",
    "        self.crop_size = crop_size\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        img  = to_pil(img).convert(\"RGB\")\n",
    "        mask = mask_to_pil(mask)\n",
    "\n",
    "        img  = TF.resize(img,  self.size, interpolation=IM.BILINEAR)\n",
    "        mask = TF.resize(mask, self.size, interpolation=IM.NEAREST)\n",
    "\n",
    "        if self.is_train and self.crop_size is not None:\n",
    "            i, j, h, w = T.RandomCrop.get_params(img, (self.crop_size, self.crop_size))\n",
    "            img  = TF.crop(img,  i, j, h, w)\n",
    "            mask = TF.crop(mask, i, j, h, w)\n",
    "            if random.random() < 0.5:\n",
    "                img  = TF.hflip(img)\n",
    "                mask = TF.hflip(mask)\n",
    "        elif self.crop_size and self.crop_size < self.size:\n",
    "            img  = TF.center_crop(img,  self.crop_size)\n",
    "            mask = TF.center_crop(mask, self.crop_size)\n",
    "\n",
    "        img  = TF.to_tensor(img)\n",
    "        img  = TF.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        mask = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        return img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2a463ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnav_vckkum5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/5] train: loss=2.7432 acc=0.351 mIoU=0.070 | val: loss=2.1826 acc=0.431 mIoU=0.099 | 6235.2s\n",
      "  ↳ Saved checkpoint: deeplabv3_resnet50_coral.pth (best val mIoU 0.099)\n",
      "[02/5] train: loss=2.0066 acc=0.457 mIoU=0.134 | val: loss=1.8074 acc=0.483 mIoU=0.134 | 4235.4s\n",
      "  ↳ Saved checkpoint: deeplabv3_resnet50_coral.pth (best val mIoU 0.134)\n",
      "[03/5] train: loss=1.7285 acc=0.489 mIoU=0.155 | val: loss=1.6883 acc=0.504 mIoU=0.141 | 4197.9s\n",
      "  ↳ Saved checkpoint: deeplabv3_resnet50_coral.pth (best val mIoU 0.141)\n",
      "[04/5] train: loss=1.6215 acc=0.506 mIoU=0.171 | val: loss=1.6886 acc=0.495 mIoU=0.144 | 4203.8s\n",
      "  ↳ Saved checkpoint: deeplabv3_resnet50_coral.pth (best val mIoU 0.144)\n",
      "[05/5] train: loss=1.5773 acc=0.515 mIoU=0.170 | val: loss=1.6858 acc=0.465 mIoU=0.145 | 4206.2s\n",
      "  ↳ Saved checkpoint: deeplabv3_resnet50_coral.pth (best val mIoU 0.145)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5 # 25\n",
    "best_miou = -1\n",
    "save_path = \"deeplabv3_resnet50_coral.pth\"\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    tr = run_epoch(model, train_loader, train=True)\n",
    "    va = run_epoch(model, val_loader, train=False)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[{epoch:02d}/{EPOCHS}] \"\n",
    "          f\"train: loss={tr['loss']:.4f} acc={tr['pix_acc']:.3f} mIoU={tr['miou']:.3f} | \"\n",
    "          f\"val: loss={va['loss']:.4f} acc={va['pix_acc']:.3f} mIoU={va['miou']:.3f} | \"\n",
    "          f\"{dt:.1f}s\")\n",
    "\n",
    "    # save best by val mIoU\n",
    "    if va[\"miou\"] > best_miou:\n",
    "        best_miou = va[\"miou\"]\n",
    "        torch.save({\"model\": model.state_dict(),\n",
    "                    \"num_classes\": num_classes}, save_path)\n",
    "        print(f\"  ↳ Saved checkpoint: {save_path} (best val mIoU {best_miou:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: loss=1.6096 acc=0.498 mIoU=0.156\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(save_path, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval();\n",
    "\n",
    "test_metrics = run_epoch(model, test_loader, train=False)\n",
    "print(f\"TEST: loss={test_metrics['loss']:.4f} acc={test_metrics['pix_acc']:.3f} mIoU={test_metrics['miou']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b2b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
